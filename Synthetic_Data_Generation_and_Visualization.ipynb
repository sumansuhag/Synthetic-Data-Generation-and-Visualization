{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MLlib acceleration: vector embeddings,‌ previously computed using a deep learning language model and stored in parquet format using array type. ‌It then uses the KMeans algorithm in Spark MLlib to cluster the vectors.   "
      ],
      "metadata": {
        "id": "_QFSXpX_SDry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PySpark library is used for big data processing and machine learning.implementing clustering algorithms like KMeans, which groups data points into clusters based on their features."
      ],
      "metadata": {
        "id": "cz8Bf2xnNGwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans clustering is an unsupervised machine learning algorithm that partitions data into K distinct clusters based on similarity."
      ],
      "metadata": {
        "id": "brq87dnaNU1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The array_to_vector function in PySpark is used to convert an array column into a vector type, specifically a DenseVector."
      ],
      "metadata": {
        "id": "CaFUkHZTNfcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.functions import array_to_vector\n",
        "from pyspark.ml.clustering import KMeans"
      ],
      "metadata": {
        "id": "ownbmav6869x"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark, Dash can be integrated to create web apps that visualize large-scale datasets processed with Spark, enabling users to share insights effectively"
      ],
      "metadata": {
        "id": "bwVR-E3LNnaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZQ9rnqxdMEMc",
        "outputId": "bf980225-0a26-4842-f32a-70451ebec698"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dash\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Werkzeug<3.1 (from dash)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash) (5.24.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\n",
            "Collecting retrying (from dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->dash) (1.17.0)\n",
            "Downloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, Werkzeug, retrying, Flask, dash\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.0\n",
            "    Uninstalling Flask-3.1.0:\n",
            "      Successfully uninstalled Flask-3.1.0\n",
            "Successfully installed Flask-3.0.3 Werkzeug-3.0.6 dash-2.18.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 retrying-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Faker library is used in PySpark to generate synthetic data for testing and development purposes. It allows users to create realistic-looking fake data,"
      ],
      "metadata": {
        "id": "MUtPOfAfNui1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1dQg6uoE8-sd",
        "outputId": "301cfa4e-2ebd-42d5-fa20-4292ddd4b101"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-36.2.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Downloading faker-36.2.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-36.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dbldatagen library is a Python tool designed for generating synthetic data within the Databricks environment using Spark."
      ],
      "metadata": {
        "id": "m2pR3OLjN5r6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dbldatagen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5Lh0UJl9gDf",
        "outputId": "348dbdd5-7018-4bc5-a9b7-57e17ae2affb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dbldatagen in /usr/local/lib/python3.11/dist-packages (0.4.0.post1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "import pandas as pd\n",
        "\n",
        "fake = Faker()\n",
        "data = []\n",
        "\n",
        "for _ in range(1000):  # Generate 1000 records\n",
        "    data.append({\n",
        "        'name': fake.name(),\n",
        "        'address': fake.address(),\n",
        "        'email': fake.email(),\n",
        "        'date_of_birth': fake.date_of_birth()\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "Fyoq_nbE9mFL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jmespath"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZZpXr9W9zit",
        "outputId": "b6480a2b-019c-445e-e5a4-df74cda7dc1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jmespath\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: jmespath\n",
            "Successfully installed jmespath-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Key Components**:\n",
        "  - **Schema Definition**: Specifies the types of data for each column.\n",
        "  - **Constraints**: Ensures that the generated data meets specific criteria (e.g., age between 18 and 65).\n",
        "  - **DataFrame Creation**: The `build()` method generates the DataFrame based on the defined schema and constraints.\n",
        "\n",
        "- **Performance Considerations**:\n",
        "  - Adjust the `partitions` parameter based on your Spark cluster configuration for optimal performance.\n",
        "  - Monitor the generated data to ensure it meets your testing requirements."
      ],
      "metadata": {
        "id": "OJLW0abr-M5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "import string\n",
        "import dbldatagen as dg\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
      ],
      "metadata": {
        "id": "qOEBzTyiLVb4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SyntheticDataGeneration\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"email\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Define the data generator with the schema and constraints\n",
        "data_gen = (\n",
        "    dg.DataGenerator(spark, name=\"SyntheticData\", rows=1000, partitions=1)\n",
        "    .withSchema(schema)\n",
        "    .withColumn(\"age\", \"int\", minValue=18, maxValue=65)\n",
        ")\n",
        "\n",
        "# Build the DataFrame with the specified schema and constraints\n",
        "df = data_gen.build()\n",
        "\n",
        "# Show the generated DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmDFK2aM9r4Y",
        "outputId": "25caa67d-bc12-47a9-ffd0-31cb8d0ad0b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+---+\n",
            "|name|age|email|age|\n",
            "+----+---+-----+---+\n",
            "|   0| 18|    0| 18|\n",
            "|   1| 19|    1| 19|\n",
            "|   2| 20|    2| 20|\n",
            "|   3| 21|    3| 21|\n",
            "|   4| 22|    4| 22|\n",
            "|   5| 23|    5| 23|\n",
            "|   6| 24|    6| 24|\n",
            "|   7| 25|    7| 25|\n",
            "|   8| 26|    8| 26|\n",
            "|   9| 27|    9| 27|\n",
            "|  10| 28|   10| 28|\n",
            "|  11| 29|   11| 29|\n",
            "|  12| 30|   12| 30|\n",
            "|  13| 31|   13| 31|\n",
            "|  14| 32|   14| 32|\n",
            "|  15| 33|   15| 33|\n",
            "|  16| 34|   16| 34|\n",
            "|  17| 35|   17| 35|\n",
            "|  18| 36|   18| 36|\n",
            "|  19| 37|   19| 37|\n",
            "+----+---+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SyntheticDataGeneration\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define a UDF to generate random strings\n",
        "def random_string(length):\n",
        "    return \"\".join(random.choice(string.ascii_lowercase) for _ in range(length))\n",
        "\n",
        "# Register the UDF\n",
        "random_string_udf = F.udf(random_string, StringType())\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"address\", StringType(), True),\n",
        "    StructField(\"phone\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create a DataFrame with the specified schema\n",
        "df = spark.createDataFrame([], schema)\n",
        "\n",
        "# Add columns to the DataFrame using the UDF\n",
        "df = df.withColumn(\"age\", F.lit(0))  # Initialize age column\n",
        "df = df.withColumn(\"age\", F.col(\"age\") + F.rand() * 47 + 18)  # Randomize age between 18 and 65\n",
        "df = df.withColumn(\"email\", random_string_udf(F.lit(50)))\n",
        "df = df.withColumn(\"address\", random_string_udf(F.lit(100)))\n",
        "df = df.withColumn(\"phone\", random_string_udf(F.lit(20)))\n",
        "\n",
        "# Show the generated DataFrame\n",
        "df.show()\n",
        "\n",
        "# Write the DataFrame to a Parquet file\n",
        "df.write.mode(\"overwrite\").parquet(\"synthetic_data.parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRz1thNDJiDv",
        "outputId": "b236f6a0-64bf-4ad5-b3af-3f242702ebff"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+-----+-------+-----+\n",
            "|name|age|email|address|phone|\n",
            "+----+---+-----+-------+-----+\n",
            "+----+---+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType, IntegerType, StructType, StructField\n",
        "from pyspark.sql import SparkSession\n",
        "import random\n",
        "import string"
      ],
      "metadata": {
        "id": "c0_p-lU9OKTT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SyntheticDataGeneration\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define a UDF to generate random strings\n",
        "def random_string(length):\n",
        "    return \"\".join(random.choice(string.ascii_lowercase) for _ in range(length))\n",
        "\n",
        "# Register the UDF\n",
        "random_string_udf = F.udf(random_string, StringType())\n",
        "\n",
        "# Define the schema\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"email\", StringType(), True),\n",
        "    StructField(\"address\", StringType(), True),\n",
        "    StructField(\"phone\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create a DataFrame with the specified schema\n",
        "df = spark.createDataFrame([], schema)\n",
        "\n",
        "# Add columns to the DataFrame using the UDF\n",
        "df = df.withColumn(\"age\", F.lit(0))  # Initialize age column\n",
        "df = df.withColumn(\"age\", F.col(\"age\") + F.rand() * 47 + 18)  # Randomize age between 18 and 65\n",
        "df = df.withColumn(\"email\", random_string_udf(F.lit(50)))\n",
        "df = df.withColumn(\"address\", random_string_udf(F.lit(100)))\n",
        "df = df.withColumn(\"phone\", random_string_udf(F.lit(20)))\n",
        "\n",
        "# Convert the DataFrame to a Pandas DataFrame\n",
        "pdf = df.toPandas()\n",
        "\n",
        "# Create a Dash application\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Define the layout of the application\n",
        "app.layout = html.Div([\n",
        "    html.H1('Synthetic Data Dashboard'),\n",
        "    html.P('Select a column to display:'),\n",
        "    dcc.Dropdown(\n",
        "        id='column-dropdown',\n",
        "        options=[\n",
        "            {'label': 'Age', 'value': 'age'},\n",
        "            {'label': 'Email', 'value': 'email'},\n",
        "            {'label': 'Address', 'value': 'address'},\n",
        "            {'label': 'Phone', 'value': 'phone'}\n",
        "        ],\n",
        "        value='age'\n",
        "    ),\n",
        "    dcc.Graph(id='column-graph')\n",
        "])\n",
        "\n",
        "# Define a callback function to update the graph\n",
        "@app.callback(\n",
        "    Output('column-graph', 'figure'),\n",
        "    [Input('column-dropdown', 'value')]\n",
        ")\n",
        "def update_graph(selected_column):\n",
        "    fig = px.histogram(pdf, x=selected_column, title=f'Distribution of {selected_column.capitalize()}')\n",
        "    return fig\n",
        "\n",
        "# Run the application\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "bIffx3cJL-q-",
        "outputId": "a6a28eed-5464-40e0-c936-458acf69b77e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}